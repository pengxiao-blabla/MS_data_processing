{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623cadb5",
   "metadata": {},
   "source": [
    "# 这个文件处理：\n",
    "这段数据的模拟时间是600$\\Omega_{cp}^{-1}$ ,将数据截断至450$\\Omega_{cp}^{-1}$\n",
    "\n",
    "scalar数据记录：\n",
    "1. 读取scalar.txt的量，并记录电磁场，U_tot， Ukin_ions的能量密度 （ E（t) ）；\n",
    "\n",
    "2. 读取particle binning的数据，记录三种粒子的垂直，平行温度以及A（温度各向异性）\n",
    "\n",
    "field_0数据记录：\n",
    "1. 读取field0，数据空间间隔，x轴y轴分别间隔原来两倍，数据量会缩小原来的4倍，后续用作分析波形特点，频率和空间波数\n",
    "\n",
    "field_1数据记录：\n",
    "1. 读取field0，数据重采样的时间间隔成原来的10倍，数据量会缩小成原来的10倍，后续用作空间滤波"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213afa71-fac2-4553-8271-36ba324b04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerics\n",
    "import numpy as np\n",
    "import math as m\n",
    "import scipy\n",
    "from scipy.fftpack import fft,ifft,fftshift,fft2\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy import signal\n",
    "pi = m.pi\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import MultipleLocator, AutoMinorLocator\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.patches import FancyArrowPatch, ArrowStyle\n",
    "import matplotlib.patches as patches\n",
    "from wav_analysis.add_cb import add_color_bar_V\n",
    "from cmap import purula\n",
    "\n",
    "# others\n",
    "import os\n",
    "import happi\n",
    "import h5py\n",
    "\n",
    "plt.style.use(\"paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d9a259-cf9f-44e5-8566-1b7c7b095bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded simulation '/media/ustcxp/C14D581BDA18EBFA/research/MS_wave/para_MS/ns/20ns'\n",
      "Scanning for Scalar diagnostics\n",
      "Scanning for Field diagnostics\n",
      "Scanning for Probe diagnostics\n",
      "Scanning for ParticleBinning diagnostics\n",
      "Scanning for RadiationSpectrum diagnostics\n",
      "Scanning for Performance diagnostics\n",
      "Scanning for Screen diagnostics\n",
      "Scanning for Tracked particle diagnostics\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "# Open a Scalar diagnostic\n",
    "S = happi.Open(path)\n",
    "filename = '20ns_scalar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba719d6d-a126-4d48-a193-769cb85d865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic parameters\n",
    "me = 1.\n",
    "e = 1.\n",
    "mp = 100 * me\n",
    "c = 1\n",
    "# -> wpe = 1\n",
    "\n",
    "wpp = 1./np.sqrt(mp)  # proton plasma frequency\n",
    "v_ratio = 15.         # c/vA\n",
    "wcp = wpp/v_ratio     # cyclotron frequency\n",
    "B0 = wcp*mp/e         # magnetic field magnitude\n",
    "\n",
    "vA = c/v_ratio        # proton Afven speed\n",
    "vbth = 0.03*vA        # background proton thermal velocity\n",
    "vsth = 0.45*vA        # shell dist. proton thermal velocity\n",
    "dt = 0.001            # simualtion time step, in wcp^-1\n",
    "\n",
    "#使用初始背景磁场的大小来初步验证模拟结果\n",
    "init_Bx = B0                     # B0 is on x axis\n",
    "lambda_p = c/wpp             # characteristc length (proton inertial length)\n",
    "\n",
    "# simulation length & grid \n",
    "Lx = 102.4*lambda_p         \n",
    "Ly = 12.8*lambda_p\n",
    "Nx = 512\n",
    "Ny = 128\n",
    "dx = Lx/Nx\n",
    "dy = Ly/Ny\n",
    "\n",
    "#setting grid\n",
    "#空间上的单位是lambda_i\n",
    "dx_norm = dx/lambda_p\n",
    "dy_norm = dy/lambda_p\n",
    "x = np.linspace(0,Nx,Nx+1)\n",
    "y = np.linspace(0,Ny,Ny+1)\n",
    "\n",
    "X,Y = np.meshgrid(x.astype(int),y.astype(int))\n",
    "\n",
    "\n",
    "# 读取scalar.txt中的Bx, 计算初始磁场总能量来进行验算\n",
    "UBx = S.Scalar('Uelm_Bx_m').getData()\n",
    "Utot = S.Scalar('Utot').getData()\n",
    "init_U = Utot[0]\n",
    "\n",
    "# integrate \n",
    "init_UB = 0.5 * init_Bx**2 * dx * dy * (Nx+1) * (Ny+2)\n",
    "\n",
    "print('Uelm_Bx_m[t=0] = ', {UBx[0]})\n",
    "print('integrate inital UB = ', {init_UB})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a93d4-f080-40e7-9851-dc1066e086e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取scalar.txt中的By, Bz, Ex, Ey, Ez, Ukin_shell_ion, Ukin_cold_ion\n",
    "\n",
    "wcp_unit = '$\\Omega_{cp}$'\n",
    "# every_field0 = 50\n",
    "Bx = np.array(S.Field(0,'Bx').getData())\n",
    "field0_t = np.array(S.Field(0,'Bx').getTimes())\n",
    "\n",
    "# unit transfer to \\Omega_cp\n",
    "# field0_t * wcp\n",
    "\n",
    "# every_field1 = 500\n",
    "UBy = S.Scalar('Uelm_By_m').getData()/init_UB\n",
    "UBz = S.Scalar('Uelm_Bz_m').getData()/init_UB\n",
    "\n",
    "UEx = S.Scalar('Uelm_Ex').getData()/(c**2*init_UB)\n",
    "UEy = S.Scalar('Uelm_Ey').getData()/(c**2*init_UB)\n",
    "UEz = S.Scalar('Uelm_Ez').getData()/(c**2*init_UB)\n",
    "field1_t = np.array(S.Scalar('Uelm_By_m').getTimes())\n",
    "\n",
    "\n",
    "# particles energy\n",
    "Ukin_shell_ion = S.Scalar('Ukin_shell_ion').getData()\n",
    "Ukin_cold_ion = S.Scalar('Ukin_cold_ion').getData()\n",
    "\n",
    "\n",
    "# 单独计算 \\delta Bx\n",
    "# UBx\n",
    "UBx = np.zeros(len(field0_t))\n",
    "delta_Bx = Bx - B0        \n",
    "\n",
    "for i in range(len(field0_t)):\n",
    "    UBx[i] = np.sum(0.5 * np.power(delta_Bx[i,:,:],2)*dx*dy)\n",
    "UBx /= init_UB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076cbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据截断至450\n",
    "# 转换时间单位\n",
    "t_snapshot = 450\n",
    "t0_normalized = field0_t * wcp\n",
    "t1_normalized = field1_t * wcp\n",
    "\n",
    "# 2. 找到时间等于 450 对应的索引位置\n",
    "# 使用 np.searchsorted 可以高效找到最接近 450 的索引\n",
    "idx0_s = np.searchsorted(t0_normalized, t_snapshot)\n",
    "idx1_s = np.searchsorted(t1_normalized, t_snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd1ff6-f3ef-44d1-b8e4-1038052358a1",
   "metadata": {},
   "source": [
    "# 1. Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576681c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数组进行切片截断 [0:idx]\n",
    "# 截断 Field 相关数据 (Bx, UBx, field0_t)\n",
    "field0_t = field0_t[:idx0_s]\n",
    "UBx = UBx[:idx0_s]\n",
    "# 如果 Bx 很大，截断它能节省内存\n",
    "Bx = Bx[:idx0_s, :, :] \n",
    "\n",
    "# 截断 Scalar 相关数据 (UBy, UBz, UEx, UEy, UEz, Ukin, field1_t)\n",
    "field1_t = field1_t[:idx1_s]\n",
    "UBy = UBy[:idx1_s]\n",
    "UBz = UBz[:idx1_s]\n",
    "UEx = UEx[:idx1_s]\n",
    "UEy = UEy[:idx1_s]\n",
    "UEz = UEz[:idx1_s]\n",
    "Ukin_shell_ion = Ukin_shell_ion[:idx1_s]\n",
    "Ukin_cold_ion = Ukin_cold_ion[:idx1_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47385ab0-0913-43c7-a1c2-a188ed1d7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./data/' + filename + '_scalar.h5','a') as f:\n",
    "    f.create_dataset('field0_t', data=field0_t)\n",
    "    f.create_dataset('field1_t', data=field1_t)\n",
    "    f.create_dataset('UBx', data=UBx)\n",
    "    f.create_dataset('UBy', data=UBy)\n",
    "    f.create_dataset('UBz', data=UBz)\n",
    "    f.create_dataset('UEx', data=UEx)\n",
    "    f.create_dataset('UEy', data=UEy)\n",
    "    f.create_dataset('UEz', data=UEz)\n",
    "    f.create_dataset('Ukin_shell_ion ', data=Ukin_shell_ion)\n",
    "    f.create_dataset('Ukin_cold_ion ', data=Ukin_cold_ion)\n",
    "\n",
    "del UBx,UBy,UBz,UEx,UEy,UEz,Utot,Ukin_cold_ion,Ukin_shell_ion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c47a1-4c42-4b15-82df-d63a46453299",
   "metadata": {},
   "source": [
    "# 2. Particle Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb7622dc-8f25-4368-aed0-9e3502fbc5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "T0 = mp*vbth**2\n",
    "\n",
    "Pxx_c = np.array(S.ParticleBinning(0).getData())\n",
    "Pyy_c = np.array(S.ParticleBinning(1).getData())\n",
    "Pzz_c = np.array(S.ParticleBinning(2).getData())\n",
    "ntot_c = np.array(S.ParticleBinning(3).getData())\n",
    "\n",
    "Pxx_s = np.array(S.ParticleBinning(4).getData())\n",
    "Pyy_s = np.array(S.ParticleBinning(5).getData())\n",
    "Pzz_s = np.array(S.ParticleBinning(6).getData())\n",
    "ntot_s = np.array(S.ParticleBinning(7).getData())\n",
    "\n",
    "Pxx_e = np.array(S.ParticleBinning(8).getData())\n",
    "Pyy_e = np.array(S.ParticleBinning(9).getData())\n",
    "Pzz_e = np.array(S.ParticleBinning(10).getData())\n",
    "ntot_e = np.array(S.ParticleBinning(11).getData())\n",
    "\n",
    "binning_t = np.array(S.ParticleBinning(11).getTimes())\n",
    "\n",
    "Tpa_c = Pxx_c/ntot_c / T0\n",
    "Tpa_s = Pxx_s/ntot_s / T0\n",
    "Tpa_e = Pxx_e/ntot_e / T0\n",
    "\n",
    "Tpe_c = (Pyy_c+Pzz_c)/ntot_c/2 / T0 \n",
    "Tpe_s = (Pyy_s+Pzz_s)/ntot_s/2 / T0 \n",
    "Tpe_e = (Pyy_e+Pzz_e)/ntot_e/2 / T0 \n",
    "\n",
    "A_c = Tpe_c/Tpa_c - 1\n",
    "A_s = Tpe_s/Tpa_s - 1\n",
    "A_e = Tpe_e/Tpa_e - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f225a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据截断至450\n",
    "# 转换时间单位\n",
    "t_snapshot = 450\n",
    "t_binning_normalized = binning_t * wcp\n",
    "\n",
    "\n",
    "# 找到时间等于 450 对应的索引位置\n",
    "# every_particleBinning = 100\n",
    "# 使用 np.searchsorted 可以高效找到最接近 450 的索引\n",
    "idx_binning_s = np.searchsorted(t_binning_normalized, t_snapshot)\n",
    "\n",
    "# 对数组进行切片截断 [0:idx]\n",
    "# 截断 particle binning 相关数据 (binning_t, Tpa_c, Tpe_c, A_c)\n",
    "binning_t = binning_t[:idx_binning_s]\n",
    "Tpa_c = UBy[:idx_binning_s]\n",
    "Tpe_c = UBz[:idx_binning_s]\n",
    "A_c = UEx[:idx_binning_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd9a44f-3456-408e-8670-ee980f489499",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./data/' + filename + '_scalar.h5','a') as f:\n",
    "    f.create_dataset('binning_t', data=binning_t)\n",
    "    f.create_dataset('Tpa_c', data=Tpa_c)\n",
    "    f.create_dataset('Tpe_c', data=Tpe_c)\n",
    "    f.create_dataset('A_c', data=A_c)\n",
    "\n",
    "del Tpa_c,Tpa_s,Tpa_e,Tpe_c,Tpe_s,Tpe_e,A_c,A_s,A_e\n",
    "del Pxx_c,Pxx_s,Pxx_e\n",
    "del Pyy_c,Pyy_s,Pyy_e\n",
    "del Pzz_c,Pzz_s,Pzz_e\n",
    "del ntot_c,ntot_s,ntot_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1461c4f-549a-4d5f-93ae-3525339b2f06",
   "metadata": {},
   "source": [
    "# 3. wave properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a1bdd3e-c7f8-408f-8933-e963f8333e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bx and time have been read in scalar data processing\n",
    "# read field0\n",
    "By = np.array(S.Field(0,'By').getData())\n",
    "Bz = np.array(S.Field(0,'Bz').getData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "208930dd-9a6d-4ba8-8a90-2bee8f44d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./data/ns20.h5','a') as f:\n",
    "    f.create_dataset('Bx', data=Bx)\n",
    "    f.create_dataset('By', data=By)\n",
    "    f.create_dataset('Bz', data=Bz)\n",
    "\n",
    "del bx_space,by_space,bz_space\n",
    "del xpsd_space,ypsd_space,zpsd_space\n",
    "del bx,by,bz\n",
    "del xpsd, ypsd, zpsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f50fc1-0e2b-4327-bde2-bc18955fa37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
